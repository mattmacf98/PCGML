{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13281\n",
      "13281\n",
      "0    <START> \"A\" Cell Breeding Device delim Spell C...\n",
      "1    <START> \"A\" Cell Incubator delim Spell Card de...\n",
      "2    <START> \"A\" Cell Recombination Device delim Sp...\n",
      "3    <START> \"A\" Cell Scatter Burst delim Spell Car...\n",
      "4    <START> \"Infernoble Arms - Almace\" delim Spell...\n",
      "5    <START> \"Infernoble Arms - Durendal\" delim Spe...\n",
      "6    <START> \"Infernoble Arms - Hauteclere\" delim S...\n",
      "7    <START> \"Infernoble Arms - Joyeuse\" delim Spel...\n",
      "8    <START> 1st Movement Solo delim Spell Card del...\n",
      "9    <START> 3-Hump Lacooda delim Effect Monster de...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cards.csv\", usecols=['name', 'type', 'atk', 'def', 'level', 'race'])\n",
    "df['atk'] = df['atk'].fillna(0)\n",
    "df['def'] = df['def'].fillna(0)\n",
    "df['level'] = df['level'].fillna(0)\n",
    "\n",
    "MASK_TOKEN = \"<UNK>\"\n",
    "\n",
    "\n",
    "df['text'] = df.apply(lambda row: f\"{row['name']} | {row['type']} | {int(row['atk'])} | {int(row['def'])} | {int(row['level'])} | {row['race']}\", axis=1)\n",
    "\n",
    "print(len(df['text']))\n",
    "input_entries = df['text']\n",
    "target_entries = input_entries.copy()\n",
    "print(len(input_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    <START> <UNK> Cell Breeding Device <UNK> <UNK>...\n",
      "1    <START> \"A\" Cell <UNK> delim Spell <UNK> delim...\n",
      "2    <START> \"A\" <UNK> Recombination <UNK> delim Sp...\n",
      "3    <START> \"A\" <UNK> Scatter <UNK> delim Spell Ca...\n",
      "4    <START> \"Infernoble <UNK> - Almace\" delim <UNK...\n",
      "5    <START> \"Infernoble <UNK> - Durendal\" delim <U...\n",
      "6    <START> \"Infernoble <UNK> - Hauteclere\" delim ...\n",
      "7    <START> \"Infernoble <UNK> - Joyeuse\" <UNK> <UN...\n",
      "8    <START> 1st Movement <UNK> delim Spell Card <U...\n",
      "9    <START> <UNK> Lacooda delim Effect Monster del...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "masking_prob = 0.2\n",
    "masked_entries = []\n",
    "for entry in input_entries:\n",
    "    tokens = entry.split(' ')\n",
    "    masked_entry = []\n",
    "    for token in tokens:\n",
    "        if np.random.rand() < masking_prob:\n",
    "            masked_entry.append(MASK_TOKEN)\n",
    "        else:\n",
    "            masked_entry.append(token)\n",
    "    masked_entries.append(\" \".join(masked_entry))\n",
    "\n",
    "masked_entries = pd.Series(masked_entries)\n",
    "print(masked_entries[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = np.array([masked_entries[i] + \" -> \" + target_entries[i] + \"<END>\" for i in range(len(masked_entries))])\n",
    "print(X.shape)\n",
    "print(X[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macfarqu/Desktop/My_Projects/PCGML/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/macfarqu/Desktop/My_Projects/PCGML/venv/lib/python3.9/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    device_map='auto',\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# LoRa\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")\n",
    "\n",
    "model = get_peft_model(model, config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - X: A list of input sequences (strings) for next-token prediction.\n",
    "        - tokenizer: The tokenizer for processing the text.\n",
    "        - max_length: The maximum length of the tokenized input/output.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the input sequence\n",
    "        x = self.X[idx]\n",
    "\n",
    "        # Tokenize the input sequence\n",
    "        tokenized = self.tokenizer(x.strip(), truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        input_ids = tokenized['input_ids'].squeeze()  # Remove batch dimension\n",
    "        attention_mask = tokenized['attention_mask'].squeeze()\n",
    "\n",
    "        # Labels for next-token prediction are the same as input_ids but shifted to the right\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100  # Ignore padding in the loss computation\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macfarqu/Desktop/My_Projects/PCGML/venv/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = TextDataset(X, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  18%|█▊        | 299/1661 [19:40<1:28:07,  3.88s/it, loss=1.05] "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_description(f'Epoch {epoch+1}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  <START> powerful <UNK> skeleton DELIM Ritual Effect <UNK> DELIM 1000 DELIM 3000 DELIM <UNK> DELIM fiend <END>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "input_text = \"<UNK> Magician | <UNK> | 3300 | <UNK> | 7 | <UNK> -> \"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=100,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Text: \", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}