{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macfarqu/Desktop/My_Projects/PCGML/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torchvision.transforms import transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class LinearNoiseScheduler:\n",
    "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
    "        self.sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod.to(device)\n",
    "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1. - self.alpha_cum_prod)\n",
    "        self.sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod.to(device)\n",
    "\n",
    "    def add_noise(self, original, noise, t):\n",
    "        original_shape = original.shape\n",
    "        batch_size = original_shape[0]\n",
    "\n",
    "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod[t].reshape(batch_size)\n",
    "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size)\n",
    "\n",
    "        for _ in range(len(original_shape)-1):\n",
    "            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n",
    "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
    "\n",
    "        return sqrt_alpha_cum_prod*original + sqrt_one_minus_alpha_cum_prod * noise\n",
    "\n",
    "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
    "        x0 = (xt - self.sqrt_one_minus_alpha_cum_prod[t] * noise_pred) / self.sqrt_alpha_cum_prod[t]\n",
    "        x0 = torch.clamp(x0, -1., 1.)\n",
    "\n",
    "        mean = xt - ((self.betas[t] * noise_pred) / self.sqrt_one_minus_alpha_cum_prod[t])\n",
    "        mean = mean / torch.sqrt(self.alphas[t])\n",
    "\n",
    "        if t == 0:\n",
    "            return mean, x0\n",
    "        else:\n",
    "            variance = (1 - self.alpha_cum_prod[t-1]) / (1 - self.alpha_cum_prod[t])\n",
    "            variance = variance * self.betas[t]\n",
    "            sigma = variance ** 0.5\n",
    "            z = torch.randn(xt.shape).to(xt.device)\n",
    "            return mean + sigma*z,  x0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_time_embedding(time_steps, t_emb_dim):\n",
    "    factor = 10000 ** (torch.arange(start=0, end=t_emb_dim // 2, device=time_steps.device) / (t_emb_dim // 2))\n",
    "\n",
    "    t_emb = time_steps[:, None].repeat(1, t_emb_dim // 2) / factor\n",
    "    return torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, down_sample, num_heads):\n",
    "        super().__init__()\n",
    "        self.down_sample = down_sample\n",
    "\n",
    "        self.resnet_conv_first = nn.Sequential(\n",
    "            nn.GroupNorm(8, in_channels),  # Why 8?\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1)\n",
    "        )\n",
    "\n",
    "        self.t_emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(t_emb_dim, out_channels)\n",
    "        )\n",
    "\n",
    "        self.resnet_conv_second = nn.Sequential(\n",
    "            nn.GroupNorm(8, out_channels),  # Why 8?\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1)\n",
    "        )\n",
    "\n",
    "        self.attention_norm = nn.GroupNorm(8, out_channels)\n",
    "        self.attention = nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "        self.residual_input_conv = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels, kernel_size=(4, 4), stride=(2, 2), padding=1) if self.down_sample else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "\n",
    "        # Resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first(out)\n",
    "        out = out + self.t_emb_layer(t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second(out)\n",
    "        out = out + self.residual_input_conv(resnet_input)\n",
    "\n",
    "        # Attention block\n",
    "        batch_size, channels, h, w = out.shape\n",
    "        in_attn = out.reshape(batch_size, channels, h*w)\n",
    "        in_attn = self.attention_norm(in_attn)\n",
    "        in_attn = in_attn.transpose(1, 2)\n",
    "        out_attn, _ = self.attention(in_attn, in_attn, in_attn)\n",
    "        out_attn = out_attn.transpose(1,2).reshape(batch_size, channels, h, w)\n",
    "        out = out + out_attn\n",
    "\n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MidBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resnet_conv_first = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, in_channels),  # Why 8?\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, out_channels),  # Why 8?\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.t_emb_layer = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.resnet_conv_second = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, out_channels),  # Why 8?\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, out_channels),  # Why 8?\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.attention_norm = nn.GroupNorm(8, out_channels)\n",
    "        self.attention = nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "        self.residual_input_conv = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1)),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=(1, 1))\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "\n",
    "        # first Resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        out = out + self.t_emb_layer[0](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "\n",
    "        # Attention block\n",
    "        batch_size, channels, h, w = out.shape\n",
    "        in_attn = out.reshape(batch_size, channels, h*w)\n",
    "        in_attn = self.attention_norm(in_attn)\n",
    "        in_attn = in_attn.transpose(1, 2)\n",
    "        out_attn, _ = self.attention(in_attn, in_attn, in_attn)\n",
    "        out_attn = out_attn.transpose(1,2).reshape(batch_size, channels, h, w)\n",
    "        out = out + out_attn\n",
    "\n",
    "        # second ResnetBlock\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[1](out)\n",
    "        out = out + self.t_emb_layer[1](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[1](out)\n",
    "        out = out + self.residual_input_conv[1](resnet_input)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample, num_heads):\n",
    "        super().__init__()\n",
    "        self.up_sample = up_sample\n",
    "\n",
    "        self.resnet_conv_first = nn.Sequential(\n",
    "            nn.GroupNorm(8, in_channels),  # Why 8?\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1)\n",
    "        )\n",
    "\n",
    "        self.t_emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(t_emb_dim, out_channels)\n",
    "        )\n",
    "\n",
    "        self.resnet_conv_second = nn.Sequential(\n",
    "            nn.GroupNorm(8, out_channels),  # Why 8?\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1)\n",
    "        )\n",
    "\n",
    "        self.attention_norm = nn.GroupNorm(8, out_channels)\n",
    "        self.attention = nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "        self.residual_input_conv = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)) if self.up_sample else nn.Identity()\n",
    "\n",
    "    def forward(self, x, out_down, t_emb):\n",
    "        x = self.up_sample_conv(x)\n",
    "        x = torch.cat([x, out_down], dim=1)\n",
    "        out = x\n",
    "\n",
    "        # Resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first(out)\n",
    "        out = out + self.t_emb_layer(t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second(out)\n",
    "        out = out + self.residual_input_conv(resnet_input)\n",
    "\n",
    "        # Attention block\n",
    "        batch_size, channels, h, w = out.shape\n",
    "        in_attn = out.reshape(batch_size, channels, h*w)\n",
    "        in_attn = self.attention_norm(in_attn)\n",
    "        in_attn = in_attn.transpose(1, 2)\n",
    "        out_attn, _ = self.attention(in_attn, in_attn, in_attn)\n",
    "        out_attn = out_attn.transpose(1,2).reshape(batch_size, channels, h, w)\n",
    "        out = out + out_attn\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, im_channels):\n",
    "        super().__init__()\n",
    "        self.down_channels = [32, 64, 128, 256]\n",
    "        self.mid_channels = [256, 256, 128]\n",
    "        self.up_channels = [128, 64, 32, 16]\n",
    "        self.t_emb_dim = 128\n",
    "        self.down_sample = [True, True, False]\n",
    "        self.up_sample = [False, True, True]\n",
    "\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "        )\n",
    "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=(3, 3), padding=1)\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels) - 1):\n",
    "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i+1], self.t_emb_dim, down_sample=self.down_sample[i], num_heads=4))\n",
    "\n",
    "        self.mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels) - 1):\n",
    "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i+1], self.t_emb_dim, num_heads=4))\n",
    "\n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in range(len(self.up_channels) - 1):\n",
    "            self.ups.append(UpBlock(self.up_channels[i] * 2, self.up_channels[i+1], self.t_emb_dim, up_sample=self.up_sample[i], num_heads=4))\n",
    "\n",
    "        self.norm_out = nn.GroupNorm(8, 16)\n",
    "        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        out = self.conv_in(x)\n",
    "        t_emb = get_time_embedding(t, self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "\n",
    "        down_outs = []\n",
    "        for down in self.downs:\n",
    "            down_outs.append(out)\n",
    "            out = down(out, t_emb)\n",
    "\n",
    "        for mid in self.mids:\n",
    "            out = mid(out, t_emb)\n",
    "\n",
    "        for up in self.ups:\n",
    "            down_out = down_outs.pop()\n",
    "            out = up(out, down_out, t_emb)\n",
    "\n",
    "        out = self.norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.conv_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Convert to RGB if the images are not grayscale\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 214/252 [10:20<01:50,  2.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 45\u001B[0m\n\u001B[1;32m     42\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFinished epoch:\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m | Loss: \u001B[39m\u001B[38;5;132;01m{:.4f}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(epoch_idx \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, np\u001B[38;5;241m.\u001B[39mmean(losses)))\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n\u001B[0;32m---> 45\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdiffusion_params\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnum_timesteps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbeta_start\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.0001\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbeta_end\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.02\u001B[39;49m\n\u001B[1;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel_params\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mim_channels\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mim_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m56\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m56\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain_params\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnum_epochs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m40\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnum_samples\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnum_grid_rows\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.0001\u001B[39;49m\n\u001B[1;32m     61\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m     62\u001B[0m \u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 39\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m     37\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(noise_pred, noise)\n\u001B[1;32m     38\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m---> 39\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFinished epoch:\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m | Loss: \u001B[39m\u001B[38;5;132;01m{:.4f}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(epoch_idx \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, np\u001B[38;5;241m.\u001B[39mmean(losses)))\n",
      "File \u001B[0;32m~/Desktop/My_Projects/PCGML/venv/lib/python3.9/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/My_Projects/PCGML/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def train(args):\n",
    "    diffusion_config = args['diffusion_params']\n",
    "    model_config = args['model_params']\n",
    "    train_config = args['train_params']\n",
    "\n",
    "    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'], beta_start=diffusion_config['beta_start'], beta_end=diffusion_config['beta_end'])\n",
    "\n",
    "    trfms = transforms.Compose([\n",
    "        transforms.Resize((56, 56)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    dataset = ImageDataset(image_folder=\"pokemon/\", transform=trfms)\n",
    "    data_loader = DataLoader(dataset, batch_size=train_config['batch_size'], shuffle=True)\\\n",
    "\n",
    "    model = Unet(model_config['im_channels']).to(device)\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    num_epochs = train_config['num_epochs']\n",
    "    optimizer = Adam(model.parameters(), lr=train_config['lr'])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        losses = []\n",
    "        for im in tqdm(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            im = im.float().to(device)\n",
    "\n",
    "            noise = torch.randn_like(im).to(device)\n",
    "            t = torch.randint(0, diffusion_config['num_timesteps'], (im.shape[0],)).to(device)\n",
    "\n",
    "            noisy_im = scheduler.add_noise(im, noise, t)\n",
    "            noise_pred = model(noisy_im, t)\n",
    "\n",
    "            loss = criterion(noise_pred, noise)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Finished epoch:{} | Loss: {:.4f}'.format(epoch_idx + 1, np.mean(losses)))\n",
    "    return model\n",
    "\n",
    "model = train({\n",
    "    \"diffusion_params\": {\n",
    "        \"num_timesteps\": 1000,\n",
    "        \"beta_start\": 0.0001,\n",
    "        \"beta_end\": 0.02\n",
    "    },\n",
    "    \"model_params\": {\n",
    "        \"im_channels\": 1,\n",
    "        \"im_size\": 56\n",
    "    },\n",
    "    \"train_params\": {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 40,\n",
    "        \"num_samples\": 20,\n",
    "        \"num_grid_rows\": 5,\n",
    "        \"lr\": 0.0001\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample(model, scheduler, train_config, model_config, diffusion_config):\n",
    "    r\"\"\"\n",
    "    Sample stepwise by going backward one timestep at a time.\n",
    "    We save the x0 predictions\n",
    "    \"\"\"\n",
    "    xt = torch.randn((train_config['num_samples'],\n",
    "                      model_config['im_channels'],\n",
    "                      model_config['im_size'],\n",
    "                      model_config['im_size'])).to(device)\n",
    "    for i in tqdm(reversed(range(diffusion_config['num_timesteps']))):\n",
    "        # Get prediction of noise\n",
    "        noise_pred = model(xt, torch.as_tensor(i).unsqueeze(0).to(device))\n",
    "\n",
    "        # Use scheduler to get x0 and xt-1\n",
    "        xt, _ = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(i).to(device))\n",
    "\n",
    "        # Save x0\n",
    "        ims = torch.clamp(xt, -1., 1.).detach().cpu()\n",
    "        ims = (ims + 1) / 2\n",
    "        grid = make_grid(ims, nrow=train_config['num_grid_rows'])\n",
    "        img = torchvision.transforms.ToPILImage()(grid)\n",
    "        if not os.path.exists(os.path.join('out')):\n",
    "            os.mkdir(os.path.join('out'))\n",
    "        if i % 100 == 0:\n",
    "          img.save(os.path.join('out', 'x0_{}.png'.format(i)))\n",
    "        img.close()\n",
    "\n",
    "def infer(args, model):\n",
    "    diffusion_config = args['diffusion_params']\n",
    "    model_config = args['model_params']\n",
    "    train_config = args['train_params']\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'], beta_start=diffusion_config['beta_start'], beta_end=diffusion_config['beta_end'])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sample(model, scheduler, train_config, model_config, diffusion_config)\n",
    "\n",
    "infer({\n",
    "    \"diffusion_params\": {\n",
    "        \"num_timesteps\": 1000,\n",
    "        \"beta_start\": 0.0001,\n",
    "        \"beta_end\": 0.02\n",
    "    },\n",
    "    \"model_params\": {\n",
    "        \"im_channels\": 1,\n",
    "        \"im_size\": 56\n",
    "    },\n",
    "    \"train_params\": {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 40,\n",
    "        \"num_samples\": 20,\n",
    "        \"num_grid_rows\": 5,\n",
    "        \"lr\": 0.0001\n",
    "    }\n",
    "}, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}