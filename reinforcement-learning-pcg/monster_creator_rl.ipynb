{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  \n",
    "        self.fc2 = nn.Linear(128, 128)      \n",
    "        self.fc3 = nn.Linear(128, output_dim)     \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma, epsilon, epsilon_decay, buffer_size):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.model = DQN(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_dim)\n",
    "        q_values = self.model(torch.tensor(state, dtype=torch.float32))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "            q_values = self.model(state)\n",
    "            next_q_values = self.model(next_state)\n",
    "\n",
    "            target = reward + (self.gamma * torch.max(next_q_values).item()) if not done else reward\n",
    "        \n",
    "\n",
    "            target_f = self.model(state)\n",
    "            \n",
    "            target_f[action] = target\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = nn.MSELoss()(q_values, target_f)\n",
    "        \n",
    "            # Total loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monster_gen_world import MonsterWorldGenEnv\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "register(\n",
    "     id=\"gym_examples/MonsterWorldGenEnv-v0\",\n",
    "     entry_point=\"monster_gen_world:MonsterWorldGenEnv\",\n",
    "     max_episode_steps=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: -153.80000000000007 Total Steps: 200 Agent Epsilon: 0.9641492930601405\n",
      "Episode: 2, Total Reward: -200.0 Total Steps: 200 Agent Epsilon: 0.8723765399749112\n",
      "Episode: 3, Total Reward: -186.17999999999998 Total Steps: 200 Agent Epsilon: 0.7893391956790312\n",
      "Episode: 4, Total Reward: -164.25999999999996 Total Steps: 200 Agent Epsilon: 0.7142057784510548\n",
      "Episode: 5, Total Reward: -199.8 Total Steps: 200 Agent Epsilon: 0.6462239513319374\n",
      "Episode: 6, Total Reward: -196.58 Total Steps: 200 Agent Epsilon: 0.5847129887141925\n",
      "Episode: 7, Total Reward: -181.81999999999996 Total Steps: 200 Agent Epsilon: 0.5290569600003411\n",
      "Episode: 8, Total Reward: -200.0 Total Steps: 200 Agent Epsilon: 0.47869856207627054\n",
      "Episode: 9, Total Reward: -200.0 Total Steps: 200 Agent Epsilon: 0.4331335388419069\n",
      "Episode: 10, Total Reward: -33.45999999999999 Total Steps: 73 Agent Epsilon: 0.41760539527506135\n",
      "Episode: 11, Total Reward: -200.0 Total Steps: 200 Agent Epsilon: 0.37785553796199073\n",
      "Episode: 12, Total Reward: -67.59999999999998 Total Steps: 92 Agent Epsilon: 0.36086374305909624\n",
      "Episode: 13, Total Reward: -197.23999999999987 Total Steps: 200 Agent Epsilon: 0.32651485183700946\n",
      "Episode: 14, Total Reward: -154.59999999999997 Total Steps: 200 Agent Epsilon: 0.29543546704464907\n",
      "Episode: 15, Total Reward: -178.3599999999999 Total Steps: 200 Agent Epsilon: 0.2673143800253827\n",
      "Episode: 16, Total Reward: -200.0 Total Steps: 200 Agent Epsilon: 0.24187000458396357\n",
      "Episode: 17, Total Reward: -135.70000000000002 Total Steps: 200 Agent Epsilon: 0.2188475573663175\n",
      "Episode: 18, Total Reward: -199.61999999999998 Total Steps: 200 Agent Epsilon: 0.19801650662547338\n",
      "Episode: 19, Total Reward: -51.940000000000026 Total Steps: 126 Agent Epsilon: 0.18592337728597852\n",
      "Episode: 20, Total Reward: -117.02 Total Steps: 142 Agent Epsilon: 0.17317746584521543\n",
      "Episode: 21, Total Reward: -64.46 Total Steps: 108 Agent Epsilon: 0.16407167554319613\n",
      "Episode: 22, Total Reward: -196.10000000000002 Total Steps: 200 Agent Epsilon: 0.14845447862536687\n",
      "Episode: 23, Total Reward: -196.74 Total Steps: 200 Agent Epsilon: 0.13432380787826645\n",
      "Episode: 24, Total Reward: -45.979999999999976 Total Steps: 66 Agent Epsilon: 0.1299623910413699\n",
      "Episode: 25, Total Reward: -74.1 Total Steps: 90 Agent Epsilon: 0.12424232036850384\n",
      "Episode: 26, Total Reward: -167.99999999999994 Total Steps: 184 Agent Epsilon: 0.11331945295302485\n",
      "Episode: 27, Total Reward: -11.540000000000006 Total Steps: 44 Agent Epsilon: 0.11085303839594605\n",
      "Episode: 28, Total Reward: 7.319999999999999 Total Steps: 17 Agent Epsilon: 0.10991454716684576\n",
      "Episode: 29, Total Reward: -56.56 Total Steps: 89 Agent Epsilon: 0.10512941249920255\n",
      "Episode: 30, Total Reward: -177.47999999999988 Total Steps: 200 Agent Epsilon: 0.09512264727650294\n",
      "Episode: 31, Total Reward: -200.0 Total Steps: 200 Agent Epsilon: 0.08606837810454453\n",
      "Episode: 32, Total Reward: -30.919999999999987 Total Steps: 67 Agent Epsilon: 0.08323215044309698\n",
      "Episode: 33, Total Reward: -4.919999999999998 Total Steps: 22 Agent Epsilon: 0.08232138746070643\n",
      "Episode: 34, Total Reward: -91.92000000000002 Total Steps: 113 Agent Epsilon: 0.07779808497409992\n",
      "Episode: 35, Total Reward: -10.580000000000002 Total Steps: 26 Agent Epsilon: 0.07679300575199617\n",
      "Episode: 36, Total Reward: -4.859999999999998 Total Steps: 23 Agent Epsilon: 0.07591472638583\n",
      "Episode: 37, Total Reward: -120.87999999999997 Total Steps: 200 Agent Epsilon: 0.06868876720058065\n",
      "Episode: 38, Total Reward: -91.26000000000003 Total Steps: 111 Agent Epsilon: 0.06497949776541559\n",
      "Episode: 39, Total Reward: -20.580000000000002 Total Steps: 39 Agent Epsilon: 0.06372436111287084\n",
      "Episode: 40, Total Reward: -170.23999999999987 Total Steps: 200 Agent Epsilon: 0.057658744408051306\n",
      "Episode: 41, Total Reward: -174.19999999999987 Total Steps: 200 Agent Epsilon: 0.05217048470402805\n",
      "Episode: 42, Total Reward: -50.57999999999998 Total Steps: 104 Agent Epsilon: 0.04952630306589736\n",
      "Episode: 43, Total Reward: -163.71999999999994 Total Steps: 200 Agent Epsilon: 0.0448121315001382\n",
      "Episode: 44, Total Reward: -85.06000000000003 Total Steps: 125 Agent Epsilon: 0.042096443722335425\n",
      "Episode: 45, Total Reward: -26.880000000000003 Total Steps: 50 Agent Epsilon: 0.04105682213202884\n",
      "Episode: 46, Total Reward: -160.45999999999992 Total Steps: 200 Agent Epsilon: 0.0371488198889033\n",
      "Episode: 47, Total Reward: -193.9999999999998 Total Steps: 200 Agent Epsilon: 0.033612801660594174\n",
      "Episode: 48, Total Reward: -91.20000000000006 Total Steps: 174 Agent Epsilon: 0.030811415023290857\n",
      "Episode: 49, Total Reward: 3.2799999999999994 Total Steps: 20 Agent Epsilon: 0.030504760033960118\n",
      "Episode: 50, Total Reward: -143.81999999999994 Total Steps: 200 Agent Epsilon: 0.027601158039257213\n",
      "Episode: 51, Total Reward: -41.7 Total Steps: 59 Agent Epsilon: 0.026798618892521116\n",
      "Episode: 52, Total Reward: 8.48 Total Steps: 11 Agent Epsilon: 0.026651594417452858\n",
      "Episode: 53, Total Reward: -16.239999999999995 Total Steps: 44 Agent Epsilon: 0.026071518545856334\n",
      "Episode: 54, Total Reward: -63.29999999999998 Total Steps: 105 Agent Epsilon: 0.02473774823537637\n",
      "Episode: 55, Total Reward: -132.54000000000005 Total Steps: 200 Agent Epsilon: 0.022383080470714963\n",
      "Episode: 56, Total Reward: -125.25999999999996 Total Steps: 200 Agent Epsilon: 0.020252542252089042\n",
      "Episode: 57, Total Reward: -193.51999999999992 Total Steps: 200 Agent Epsilon: 0.018324799761556265\n",
      "Episode: 58, Total Reward: -67.66 Total Steps: 200 Agent Epsilon: 0.016580549845118576\n",
      "Episode: 59, Total Reward: -133.23999999999995 Total Steps: 200 Agent Epsilon: 0.015002326723548013\n",
      "Episode: 60, Total Reward: -5.060000000000004 Total Steps: 79 Agent Epsilon: 0.014421143462314264\n",
      "Episode: 61, Total Reward: -43.2 Total Steps: 63 Agent Epsilon: 0.013973847515526655\n",
      "Episode: 62, Total Reward: -73.90000000000002 Total Steps: 172 Agent Epsilon: 0.012822046099314997\n",
      "Episode: 63, Total Reward: -37.90000000000001 Total Steps: 87 Agent Epsilon: 0.012276110797590403\n",
      "Episode: 64, Total Reward: -108.75999999999999 Total Steps: 200 Agent Epsilon: 0.01110760661137831\n",
      "Episode: 65, Total Reward: -134.74 Total Steps: 200 Agent Epsilon: 0.01005032674170329\n",
      "Episode: 66, Total Reward: 9.84 Total Steps: 2 Agent Epsilon: 0.010040278927543275\n",
      "Episode: 67, Total Reward: -115.11999999999992 Total Steps: 200 Agent Epsilon: 0.009995187929535779\n",
      "Episode: 68, Total Reward: -184.4399999999999 Total Steps: 200 Agent Epsilon: 0.009995187929535779\n",
      "Episode: 69, Total Reward: -78.23999999999995 Total Steps: 145 Agent Epsilon: 0.009995187929535779\n",
      "Episode: 70, Total Reward: -137.57999999999998 Total Steps: 172 Agent Epsilon: 0.009995187929535779\n",
      "Episode: 71, Total Reward: -77.72000000000001 Total Steps: 128 Agent Epsilon: 0.009995187929535779\n",
      "Episode: 72, Total Reward: -36.17999999999998 Total Steps: 165 Agent Epsilon: 0.009995187929535779\n",
      "Episode: 73, Total Reward: -100.20000000000003 Total Steps: 200 Agent Epsilon: 0.009995187929535779\n",
      "Episode: 74, Total Reward: -93.74000000000007 Total Steps: 139 Agent Epsilon: 0.009995187929535779\n",
      "Episode: 75, Total Reward: -70.46000000000004 Total Steps: 113 Agent Epsilon: 0.009995187929535779\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and agent with Experience Replay Buffer\n",
    "env = gym.make(\"gym_examples/MonsterWorldGenEnv-v0\",)\n",
    "state_dim = 4\n",
    "action_dim = env.action_space.n\n",
    "agent = DQNAgent(state_dim, action_dim, lr=0.0001, gamma=0.99, epsilon=1.0, epsilon_decay=0.9995, buffer_size=10000)\n",
    "\n",
    "# Train the DQN agent with Experience Replay Buffer\n",
    "batch_size = 128\n",
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    while not done:\n",
    "        action = agent.act(state[\"stats\"])\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        agent.remember(state[\"stats\"], action, reward, next_state[\"stats\"], done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        agent.replay(batch_size)\n",
    "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward} Total Steps: {step} Agent Epsilon: {agent.epsilon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "reward = 0.0\n",
    "done = False\n",
    "agent.epsilon = 0.0\n",
    "while not done:\n",
    "    action = agent.act(state[\"stats\"])\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "print(state)\n",
    "print(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
