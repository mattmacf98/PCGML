{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim_1, output_dim_2):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.out_1 = nn.Linear(512, output_dim_1)\n",
    "        self.out_2 = nn.Linear(512, output_dim_2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.leaky_relu(self.fc1(x))\n",
    "        x = torch.nn.functional.leaky_relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        \n",
    "        out_1 = self.out_1(x)\n",
    "        out_2 = self.out_2(x)\n",
    "\n",
    "        return out_1, out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim_1, action_dim_2, lr, gamma, epsilon, epsilon_decay, buffer_size):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim_1 = action_dim_1\n",
    "        self.action_dim_2 = action_dim_2\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.model = DQN(state_dim, action_dim_1, action_dim_2)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_dim_1), np.random.choice(self.action_dim_2)\n",
    "        q_values_1, q_values_2 = self.model(torch.tensor(state, dtype=torch.float32))\n",
    "        return torch.argmax(q_values_1).item(), torch.argmax(q_values_2).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "            q_values_1, q_values_2 = self.model(state)\n",
    "            next_q_values_1, next_q_values_2 = self.model(next_state)\n",
    "\n",
    "            target_1 = reward + (self.gamma * torch.max(next_q_values_1).item()) if not done else reward\n",
    "            target_2 = reward + (self.gamma * torch.max(next_q_values_2).item()) if not done else reward\n",
    "\n",
    "            target_f_1, target_f_2 = self.model(state)\n",
    "            \n",
    "            target_f_1[action[0]] = target_1\n",
    "            target_f_2[action[1]] = target_2\n",
    "\n",
    "            # Compute the loss\n",
    "            loss_1 = nn.MSELoss()(q_values_1, target_f_1)\n",
    "            loss_2 = nn.MSELoss()(q_values_2, target_f_2)\n",
    "\n",
    "            # Total loss\n",
    "            loss = loss_1 + loss_2\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze_world import MazeWorldEnv\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "register(\n",
    "     id=\"gym_examples/MazeWorldEnv-v0\",\n",
    "     entry_point=\"maze_world:MazeWorldEnv\",\n",
    "     max_episode_steps=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: -42951.5 Total Steps: 250\n",
      "Episode: 2, Total Reward: -49074.0 Total Steps: 250\n",
      "Episode: 3, Total Reward: 1471.5 Total Steps: 18\n",
      "Episode: 4, Total Reward: -51108.5 Total Steps: 250\n",
      "Episode: 5, Total Reward: -43560.5 Total Steps: 250\n",
      "Episode: 6, Total Reward: -40332.0 Total Steps: 250\n",
      "Episode: 7, Total Reward: -52277.0 Total Steps: 250\n",
      "Episode: 8, Total Reward: -42754.0 Total Steps: 250\n",
      "Episode: 9, Total Reward: -20489.5 Total Steps: 132\n",
      "Episode: 10, Total Reward: -49551.0 Total Steps: 250\n",
      "Episode: 11, Total Reward: -35311.0 Total Steps: 202\n",
      "Episode: 12, Total Reward: -39436.0 Total Steps: 250\n",
      "Episode: 13, Total Reward: -50423.5 Total Steps: 250\n",
      "Episode: 14, Total Reward: 1768.0 Total Steps: 25\n",
      "Episode: 15, Total Reward: -55098.5 Total Steps: 250\n",
      "Episode: 16, Total Reward: -40698.5 Total Steps: 250\n",
      "Episode: 17, Total Reward: -53282.5 Total Steps: 250\n",
      "Episode: 18, Total Reward: -34074.0 Total Steps: 250\n",
      "Episode: 19, Total Reward: -41646.0 Total Steps: 250\n",
      "Episode: 20, Total Reward: -40685.0 Total Steps: 250\n",
      "Episode: 21, Total Reward: -47810.0 Total Steps: 250\n",
      "Episode: 22, Total Reward: -26602.0 Total Steps: 250\n",
      "Episode: 23, Total Reward: -35427.0 Total Steps: 250\n",
      "Episode: 24, Total Reward: -21309.5 Total Steps: 250\n",
      "Episode: 25, Total Reward: -50312.0 Total Steps: 250\n",
      "Episode: 26, Total Reward: -315.0 Total Steps: 59\n",
      "Episode: 27, Total Reward: 2118.0 Total Steps: 40\n",
      "Episode: 28, Total Reward: -46235.5 Total Steps: 250\n",
      "Episode: 29, Total Reward: -43021.0 Total Steps: 250\n",
      "Episode: 30, Total Reward: -30847.0 Total Steps: 250\n",
      "Episode: 31, Total Reward: -34901.0 Total Steps: 250\n",
      "Episode: 32, Total Reward: -15163.5 Total Steps: 250\n",
      "Episode: 33, Total Reward: -37447.0 Total Steps: 250\n",
      "Episode: 34, Total Reward: -56605.5 Total Steps: 250\n",
      "Episode: 35, Total Reward: -15512.0 Total Steps: 250\n",
      "Episode: 36, Total Reward: -17046.5 Total Steps: 250\n",
      "Episode: 37, Total Reward: -20815.5 Total Steps: 250\n",
      "Episode: 38, Total Reward: -49739.0 Total Steps: 250\n",
      "Episode: 39, Total Reward: -13964.0 Total Steps: 203\n",
      "Episode: 40, Total Reward: -9263.0 Total Steps: 250\n",
      "Episode: 41, Total Reward: -32797.0 Total Steps: 250\n",
      "Episode: 42, Total Reward: -7683.0 Total Steps: 250\n",
      "Episode: 43, Total Reward: 7048.0 Total Steps: 250\n",
      "Episode: 44, Total Reward: -3809.5 Total Steps: 250\n",
      "Episode: 45, Total Reward: -23364.0 Total Steps: 250\n",
      "Episode: 46, Total Reward: -41256.0 Total Steps: 250\n",
      "Episode: 47, Total Reward: 1262.0 Total Steps: 250\n",
      "Episode: 48, Total Reward: 4622.0 Total Steps: 250\n",
      "Episode: 49, Total Reward: -26315.5 Total Steps: 250\n",
      "Episode: 50, Total Reward: -16686.5 Total Steps: 250\n",
      "Episode: 51, Total Reward: 6553.5 Total Steps: 250\n",
      "Episode: 52, Total Reward: -8371.5 Total Steps: 250\n",
      "Episode: 53, Total Reward: 4098.5 Total Steps: 250\n",
      "Episode: 54, Total Reward: -20974.0 Total Steps: 250\n",
      "Episode: 55, Total Reward: 5082.5 Total Steps: 250\n",
      "Episode: 56, Total Reward: -12061.5 Total Steps: 250\n",
      "Episode: 57, Total Reward: -15815.0 Total Steps: 250\n",
      "Episode: 58, Total Reward: 3763.5 Total Steps: 250\n",
      "Episode: 59, Total Reward: 5902.5 Total Steps: 250\n",
      "Episode: 60, Total Reward: 4997.5 Total Steps: 250\n",
      "Episode: 61, Total Reward: -10032.5 Total Steps: 250\n",
      "Episode: 62, Total Reward: -30196.5 Total Steps: 250\n",
      "Episode: 63, Total Reward: 2402.5 Total Steps: 250\n",
      "Episode: 64, Total Reward: -777.5 Total Steps: 250\n",
      "Episode: 65, Total Reward: 3687.5 Total Steps: 250\n",
      "Episode: 66, Total Reward: 1757.5 Total Steps: 250\n",
      "Episode: 67, Total Reward: 2137.5 Total Steps: 250\n",
      "Episode: 68, Total Reward: 4007.5 Total Steps: 250\n",
      "Episode: 69, Total Reward: 2087.5 Total Steps: 250\n",
      "Episode: 70, Total Reward: 3092.5 Total Steps: 250\n",
      "Episode: 71, Total Reward: 3907.5 Total Steps: 250\n",
      "Episode: 72, Total Reward: 2132.5 Total Steps: 250\n",
      "Episode: 73, Total Reward: 2327.5 Total Steps: 250\n",
      "Episode: 74, Total Reward: 2477.5 Total Steps: 250\n",
      "Episode: 75, Total Reward: 1982.5 Total Steps: 250\n",
      "Episode: 76, Total Reward: 2738.5 Total Steps: 250\n",
      "Episode: 77, Total Reward: 2282.5 Total Steps: 250\n",
      "Episode: 78, Total Reward: 2917.5 Total Steps: 250\n",
      "Episode: 79, Total Reward: 3207.5 Total Steps: 250\n",
      "Episode: 80, Total Reward: 1397.5 Total Steps: 250\n",
      "Episode: 81, Total Reward: 3877.5 Total Steps: 250\n",
      "Episode: 82, Total Reward: 1392.5 Total Steps: 250\n",
      "Episode: 83, Total Reward: 1352.5 Total Steps: 250\n",
      "Episode: 84, Total Reward: 1313.5 Total Steps: 250\n",
      "Episode: 85, Total Reward: 1190.0 Total Steps: 250\n",
      "Episode: 86, Total Reward: 1392.5 Total Steps: 250\n",
      "Episode: 87, Total Reward: 1918.5 Total Steps: 250\n",
      "Episode: 88, Total Reward: 1122.5 Total Steps: 250\n",
      "Episode: 89, Total Reward: 547.5 Total Steps: 250\n",
      "Episode: 90, Total Reward: 1057.5 Total Steps: 250\n",
      "Episode: 91, Total Reward: 1702.5 Total Steps: 250\n",
      "Episode: 92, Total Reward: 1467.5 Total Steps: 250\n",
      "Episode: 93, Total Reward: 812.5 Total Steps: 250\n",
      "Episode: 94, Total Reward: 722.5 Total Steps: 250\n",
      "Episode: 95, Total Reward: 317.5 Total Steps: 250\n",
      "Episode: 96, Total Reward: 927.5 Total Steps: 250\n",
      "Episode: 97, Total Reward: 882.5 Total Steps: 250\n",
      "Episode: 98, Total Reward: 432.5 Total Steps: 250\n",
      "Episode: 99, Total Reward: 2137.5 Total Steps: 250\n",
      "Episode: 100, Total Reward: 782.5 Total Steps: 250\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and agent with Experience Replay Buffer\n",
    "size = 7\n",
    "env = gym.make(\"gym_examples/MazeWorldEnv-v0\", render_mode=\"human\", size=size)\n",
    "dimensions = [space.shape for space in list(env.observation_space.values())]\n",
    "state_dim = sum([1 if len(t) == 0 else t[0] if len(t) == 1 else t[0] * t[1] for t in dimensions])\n",
    "action_dim = np.sum([space.n for space in env.action_space])\n",
    "agent = DQNAgent(state_dim, env.action_space[0].n, env.action_space[1].n, lr=0.005, gamma=0.99, epsilon=1.0, epsilon_decay=0.999, buffer_size=10000)\n",
    "\n",
    "# Train the DQN agent with Experience Replay Buffer\n",
    "batch_size = 64\n",
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    while not done:\n",
    "        action = agent.act(np.concatenate((state[\"maze\"].reshape(size*size,), state[\"start\"], state[\"end\"])))\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        agent.remember(np.concatenate((state[\"maze\"].reshape(size*size,), state[\"start\"], state[\"end\"])), action, reward, np.concatenate((next_state[\"maze\"].reshape(size*size,), next_state[\"start\"], next_state[\"end\"])), done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        if step % 5 == 0:\n",
    "            agent.replay(batch_size)\n",
    "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward} Total Steps: {step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892.5\n"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "# agent.epsilon = 0.0\n",
    "while not done:\n",
    "    action = agent.act(np.concatenate((state[\"maze\"].reshape(size*size,), state[\"start\"], state[\"end\"])))\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "print(total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}